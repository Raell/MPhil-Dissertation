{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Dropout, Reshape\n",
    "from tensorflow.keras.utils import plot_model, Sequence\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, BinaryCrossentropy\n",
    "from gradientreversal import GradientReversal\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def grad_reverse(x):\n",
    "    y = tf.identity(x)\n",
    "    def custom_grad(dy):\n",
    "        return -dy\n",
    "    return y, custom_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    def __init__(\n",
    "        self, \n",
    "        data_folder, \n",
    "        source_domain,\n",
    "        target_domain,\n",
    "        input_shape,\n",
    "        target_labels=0.1\n",
    "    ):\n",
    "        source_gen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "        target_gen = tf.keras.preprocessing.image.ImageDataGenerator(validation_split=1-target_labels)\n",
    "        \n",
    "        self.source_data = source_gen.flow_from_directory(\n",
    "            data_folder + \"/\" + source_domain,\n",
    "            batch_size=32,\n",
    "            target_size=input_shape\n",
    "        )\n",
    "\n",
    "        self.target_data = target_gen.flow_from_directory(\n",
    "            data_folder + \"/\" + target_domain,\n",
    "            batch_size=32,\n",
    "            target_size=input_shape,\n",
    "            subset='training'\n",
    "        )\n",
    "        \n",
    "        self.valid_target = target_gen.flow_from_directory(\n",
    "            data_folder + \"/\" + target_domain,\n",
    "            batch_size=64,\n",
    "            target_size=input_shape,\n",
    "            subset='validation'\n",
    "        )\n",
    "        \n",
    "        self.classes = self.source_data.num_classes\n",
    "    def train_data(self):\n",
    "        return MergeSequence(self.source_data, self.target_data)\n",
    "            \n",
    "        \n",
    "    def valid_data(self):\n",
    "        return MergeSequence([], self.valid_target)\n",
    "\n",
    "class MergeSequence(Sequence):\n",
    "    def __init__(self, source, target):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.source) and idx < len(self.target):\n",
    "            s_img, s_class = self.source[idx]       \n",
    "            t_img, t_class = self.target[idx]\n",
    "            \n",
    "            s_class = s_class[:, np.newaxis, :]\n",
    "            s_class = np.concatenate((s_class, np.zeros(s_class.shape)), axis=1)\n",
    "            \n",
    "            t_class = t_class[:, np.newaxis, :]\n",
    "            t_class = np.concatenate((np.zeros(t_class.shape), t_class), axis=1)\n",
    "            \n",
    "            img = np.concatenate((s_img, t_img), axis=0)\n",
    "            cls = np.concatenate((s_class, t_class), axis=0)\n",
    "            \n",
    "        elif idx < len(self.source):\n",
    "            img, cls = self.source[idx] \n",
    "            cls = cls[:, np.newaxis, :]\n",
    "            cls = np.concatenate((cls, np.zeros(cls.shape)), axis=1)\n",
    "            \n",
    "        elif idx < len(self.target):\n",
    "            img, cls = self.target[idx]\n",
    "            cls = cls[:, np.newaxis, :]\n",
    "            cls = np.concatenate((np.zeros(cls.shape), cls), axis=1)\n",
    "            \n",
    "        return img, cls\n",
    "   \n",
    "    def __len__(self):\n",
    "        return max(len(self.source), len(self.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KLLoss(y_pred, classes):\n",
    "    y_joint = tf.reshape(y_pred, (-1, 2*classes))\n",
    "\n",
    "    y_domain = tf.expand_dims(tf.reduce_sum(y_pred, axis=2), -1)\n",
    "    y_class = tf.expand_dims(tf.reduce_sum(y_pred, axis=1), 1)\n",
    "    \n",
    "    y_ind_joint = tf.reshape((y_domain * y_class), (-1,2*classes))\n",
    "    return tf.keras.losses.KLDivergence()(\n",
    "            tf.transpose(y_joint), \n",
    "            tf.transpose(y_ind_joint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder(input_shape=(224, 224, 3)):\n",
    "    resnet = ResNet50(include_top=False, input_shape=input_shape)\n",
    "    pool = GlobalAveragePooling2D()(resnet.output)\n",
    "    return Model(inputs=resnet.input, outputs=pool, name=\"Encoder\")\n",
    "\n",
    "def build_classifier(input_shape, classes):\n",
    "    classifier = Sequential(name=\"Classifier\")\n",
    "    classifier.add(Dense(1280, \n",
    "                  input_shape=input_shape[1:],\n",
    "                  activation='relu'))\n",
    "    classifier.add(Dense(1280, activation='relu'))\n",
    "    classifier.add(Dense(\n",
    "        2 * classes, \n",
    "        activation='softmax',\n",
    "        name='classifier_output'\n",
    "    ))\n",
    "    classifier.add(Reshape((2, classes)))\n",
    "    return classifier\n",
    "\n",
    "def build_discriminator(input_shape):\n",
    "    discriminator = Sequential(name=\"Discriminator\")\n",
    "    discriminator.add(Dense(1280, \n",
    "                  activation='relu'))\n",
    "    discriminator.add(Dense(1280, activation='relu'))\n",
    "    discriminator.add(Dense(\n",
    "        2, \n",
    "        activation='softmax',\n",
    "        name='domain_output'\n",
    "    ))\n",
    "    discriminator.build(input_shape)\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleDomainModel(tf.keras.Model):\n",
    "    def __init__(self, \n",
    "                 encoder=None,\n",
    "                 classifier=None,\n",
    "                 use_discriminator=False,\n",
    "                 classes=65\n",
    "                ):\n",
    "        super(SingleDomainModel, self).__init__()\n",
    "        self.classes = classes\n",
    "        self.encoder = build_encoder() \\\n",
    "            if encoder is None else encoder\n",
    "        self.classifier = build_classifier(self.encoder.output_shape, self.classes) \\\n",
    "            if classifier is None else classifier\n",
    "        self.discriminator = build_discriminator(self.encoder.output_shape) \\\n",
    "            if use_discriminator else None\n",
    "        \n",
    "        self.build(self.encoder.input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        features = self.encoder(inputs)\n",
    "        classes = self.classifier(features)\n",
    "        if self.discriminator is not None:\n",
    "            domains = grad_reverse(features)\n",
    "            domains = self.discriminator(domains)\n",
    "            return classes, domains\n",
    "        else:\n",
    "            return classes\n",
    "    \n",
    "    def compile(self, optimizer=None, pred_loss=None, rep_loss=None):\n",
    "        super(SingleDomainModel, self).compile()\n",
    "        if optimizer is None:\n",
    "            lr_schedule = ExponentialDecay(\n",
    "                initial_learning_rate=1e-3,\n",
    "                decay_steps=100,\n",
    "                decay_rate=0.9)\n",
    "            self.optimizer = SGD(learning_rate=lr_schedule, momentum=0.9) \n",
    "        else:\n",
    "            self.optimizer = optimizer\n",
    "            \n",
    "        if self.discriminator is not None:\n",
    "            self.dis_loss = BinaryCrossentropy()\n",
    "            self.domain_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "            \n",
    "        if pred_loss is None:\n",
    "            self.pred_loss = CategoricalCrossentropy()\n",
    "        else:\n",
    "            self.pred_loss = pred_loss\n",
    "            \n",
    "        if rep_loss is None:\n",
    "            self.rep_loss = KLLoss\n",
    "        else:\n",
    "            self.rep_loss = rep_loss\n",
    "            \n",
    "        self.joint_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "        self.class_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        y_dom = tf.reduce_sum(y, axis=2)\n",
    "        y_class = tf.reduce_sum(y, axis=1)\n",
    "        \n",
    "        y = tf.reshape(y, (-1, 2*self.classes))\n",
    "        \n",
    "        if self.discriminator is None:    \n",
    "            # Train encoder       \n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = self(x)                \n",
    "                kl_loss = self.rep_loss(y_pred, self.classes)\n",
    "                \n",
    "                y_pred = tf.reduce_sum(y_pred, axis=1)\n",
    "                pred_loss = self.pred_loss(y_class, y_pred)\n",
    "                self.class_accuracy.update_state(y_class, y_pred)\n",
    "                \n",
    "                encoder_loss = kl_loss + pred_loss\n",
    "                                \n",
    "                grads = tape.gradient(encoder_loss, self.encoder.trainable_weights)\n",
    "                self.optimizer.apply_gradients(\n",
    "                    zip(grads, self.encoder.trainable_weights)\n",
    "                )\n",
    "\n",
    "\n",
    "            # Train classifier\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = tf.reshape(self(x), (-1, 2*self.classes))\n",
    "                self.joint_accuracy.update_state(y, y_pred)\n",
    "                joint_pred_loss = self.pred_loss(y, y_pred)\n",
    "                grads = tape.gradient(joint_pred_loss, self.classifier.trainable_weights)\n",
    "                self.optimizer.apply_gradients(\n",
    "                    zip(grads, self.classifier.trainable_weights)\n",
    "                )\n",
    "    \n",
    "            return {\n",
    "                \"loss\": encoder_loss,\n",
    "                \"kl_loss\": kl_loss,    \n",
    "                \"pred_loss\": pred_loss,\n",
    "                \"joint_pred_loss\": joint_pred_loss,   \n",
    "                \"accuracy\": self.joint_accuracy.result(),\n",
    "                \"class_accuracy\": self.class_accuracy.result()\n",
    "            }\n",
    "    \n",
    "        else:\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_joint, y_domains = self(x)               \n",
    "                kl_loss = self.rep_loss(y_joint, self.classes)\n",
    "\n",
    "                y_pred = tf.reduce_sum(y_joint, axis=1)\n",
    "                pred_loss = self.pred_loss(y_class, y_pred)\n",
    "                self.class_accuracy.update_state(y_class, y_pred)\n",
    "                \n",
    "                y_pred = tf.reshape(y_joint, (-1, 2*self.classes))\n",
    "                self.joint_accuracy.update_state(y, y_pred)\n",
    "                joint_pred_loss = self.pred_loss(y, y_pred)\n",
    "\n",
    "                dis_loss = self.dis_loss(y_dom, y_domains)\n",
    "                self.domain_accuracy.update_state(y_dom, y_domains)\n",
    "\n",
    "                encoder_loss = kl_loss + pred_loss + dis_loss\n",
    "                \n",
    "                weights = self.classifier.trainable_weights + \\\n",
    "                          self.encoder.trainable_weights + \\\n",
    "                          self.discriminator.trainable_weights\n",
    "\n",
    "                grads = tape.gradient(encoder_loss, \n",
    "                                      weights)\n",
    "                \n",
    "                self.optimizer.apply_gradients(\n",
    "                    zip(grads, weights)\n",
    "                )\n",
    "\n",
    "#             # Train encoder       \n",
    "#             with tf.GradientTape() as tape:\n",
    "#                 y_joint, y_domains = self(x)               \n",
    "#                 kl_loss = self.rep_loss(y_joint, self.classes)\n",
    "                \n",
    "#                 y_pred = tf.reduce_sum(y_joint, axis=1)\n",
    "#                 pred_loss = self.pred_loss(y_class, y_pred)\n",
    "#                 self.class_accuracy.update_state(y_class, y_pred)\n",
    "                \n",
    "#                 dis_loss = self.dis_loss(y_dom, y_domains)\n",
    "#                 self.domain_accuracy.update_state(y_dom, y_domains)      \n",
    "#                 encoder_loss = kl_loss + pred_loss + dis_loss\n",
    "#                 weights = self.encoder.trainable_weights\n",
    "#                 grads = tape.gradient(encoder_loss, weights)              \n",
    "#                 self.optimizer.apply_gradients(\n",
    "#                     zip(grads, weights)\n",
    "#                 )\n",
    "\n",
    "\n",
    "#             # Train classifier\n",
    "#             with tf.GradientTape() as tape:\n",
    "#                 y_joint, _ = self(x)\n",
    "#                 y_pred = tf.reshape(y_joint, (-1, 2*self.classes))\n",
    "#                 self.joint_accuracy.update_state(y, y_pred)\n",
    "#                 joint_pred_loss = self.pred_loss(y, y_pred)\n",
    "#                 grads = tape.gradient(joint_pred_loss, self.classifier.trainable_weights)\n",
    "#                 self.optimizer.apply_gradients(\n",
    "#                     zip(grads, self.classifier.trainable_weights)\n",
    "#                 )\n",
    "                \n",
    "#             with tf.GradientTape() as tape:\n",
    "#                 _, y_domains = self(x)                           \n",
    "#                 dis_loss = self.dis_loss(y_dom, y_domains)\n",
    "#                 self.domain_accuracy.update_state(y_dom, y_domains)\n",
    "#                 weights = self.discriminator.trainable_weights\n",
    "#                 grads = tape.gradient(dis_loss, \n",
    "#                                       weights)  \n",
    "#                 self.optimizer.apply_gradients(\n",
    "#                     zip(grads, weights)\n",
    "#                 )\n",
    "                \n",
    "            return {\n",
    "                \"loss\": encoder_loss,\n",
    "                \"kl_loss\": kl_loss,\n",
    "                \"pred_loss\": pred_loss,\n",
    "                \"dis_loss\": dis_loss, \n",
    "                \"joint_pred_loss\": joint_pred_loss,                  \n",
    "                \"accuracy\": self.joint_accuracy.result(),\n",
    "                \"class_accuracy\": self.class_accuracy.result(),\n",
    "                \"domain_accuracy\": self.domain_accuracy.result(),\n",
    "            }\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        y_dom = tf.reduce_sum(y, axis=2)\n",
    "        y_class = tf.reduce_sum(y, axis=1)\n",
    "        \n",
    "        y = tf.reshape(y, (-1, 2*self.classes))\n",
    "        \n",
    "        if self.discriminator is None:   \n",
    "            y_pred = self(x)  \n",
    "            y_class_pred = tf.reduce_sum(y_joint, axis=1)\n",
    "            y_joint_pred = tf.reshape(y_pred, (-1, 2*self.classes)) \n",
    "            \n",
    "            kl_loss = self.rep_loss(y_pred, self.classes)\n",
    "            pred_loss = self.pred_loss(y_class, y_class_pred)\n",
    "            self.class_accuracy.update_state(y_class, y_class_pred)\n",
    "            encoder_loss = kl_loss + pred_loss\n",
    "            \n",
    "            joint_pred_loss = self.pred_loss(y, y_joint_pred)\n",
    "            self.joint_accuracy.update_state(y, y_pred)\n",
    "            \n",
    "            return {\n",
    "                \"loss\": encoder_loss,\n",
    "                \"kl_loss\": kl_loss,    \n",
    "                \"pred_loss\": pred_loss,   \n",
    "                \"joint_pred_loss\": joint_pred_loss,   \n",
    "                \"accuracy\": self.joint_accuracy.result(),\n",
    "                \"class_accuracy\": self.class_accuracy.result()\n",
    "            }\n",
    "            \n",
    "        else:\n",
    "            y_joint, y_domains = self(x)\n",
    "            y_class_pred = tf.reduce_sum(y_joint, axis=1)\n",
    "            y_joint_pred = tf.reshape(y_joint, (-1, 2*self.classes)) \n",
    "            \n",
    "            \n",
    "            kl_loss = self.rep_loss(y_joint, self.classes)           \n",
    "            pred_loss = self.pred_loss(y_class, y_class_pred)\n",
    "            self.class_accuracy.update_state(y_class, y_class_pred)\n",
    "            dis_loss = self.dis_loss(y_dom, y_domains)\n",
    "            self.domain_accuracy.update_state(y_dom, y_domains)\n",
    "            encoder_loss = kl_loss + pred_loss + dis_loss\n",
    "            \n",
    "            \n",
    "            joint_pred_loss = self.pred_loss(y, y_joint_pred)\n",
    "            self.joint_accuracy.update_state(y, y_joint_pred)\n",
    "\n",
    "        return {\n",
    "            \"loss\": encoder_loss,\n",
    "            \"kl_loss\": kl_loss,    \n",
    "            \"pred_loss\": pred_loss, \n",
    "            \"dis_loss\": dis_loss,\n",
    "            \"joint_pred_loss\": joint_pred_loss,   \n",
    "            \"accuracy\": self.joint_accuracy.result(),\n",
    "            \"class_accuracy\": self.class_accuracy.result(),\n",
    "            \"domain_accuracy\": self.domain_accuracy.result(),\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4357 images belonging to 65 classes.\n",
      "Found 1361 images belonging to 65 classes.\n",
      "Found 3078 images belonging to 65 classes.\n"
     ]
    }
   ],
   "source": [
    "d = DataGenerator(\n",
    "    \"../Datasets/OfficeHomeDataset_10072016\",\n",
    "    \"Real World\",\n",
    "    \"Product\",\n",
    "    (224, 224),\n",
    "    0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"single_domain_model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Encoder (Functional)         (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "Classifier (Sequential)      (None, 2, 65)             4428930   \n",
      "_________________________________________________________________\n",
      "Discriminator (Sequential)   (None, 2)                 4264962   \n",
      "_________________________________________________________________\n",
      "categorical_accuracy (Catego multiple                  2         \n",
      "_________________________________________________________________\n",
      "categorical_accuracy (Catego multiple                  2         \n",
      "_________________________________________________________________\n",
      "categorical_accuracy (Catego multiple                  2         \n",
      "=================================================================\n",
      "Total params: 32,281,610\n",
      "Trainable params: 32,228,484\n",
      "Non-trainable params: 53,126\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m = SingleDomainModel(classes=d.classes, use_discriminator=True)\n",
    "m.compile()\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=1,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "137/137 [==============================] - 113s 824ms/step - loss: 28.7719 - kl_loss: 0.0000e+00 - pred_loss: 15.8073 - dis_loss: 12.9646 - joint_pred_loss: 16.0633 - accuracy: 0.0047 - class_accuracy: 0.0188 - domain_accuracy: 0.2290 - val_loss: 16.1181 - val_kl_loss: 0.0000e+00 - val_pred_loss: 16.1181 - val_dis_loss: 0.0000e+00 - val_joint_pred_loss: 16.1181 - val_accuracy: 0.0224 - val_class_accuracy: 0.0224 - val_domain_accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "137/137 [==============================] - 116s 845ms/step - loss: 28.7683 - kl_loss: 0.0000e+00 - pred_loss: 15.8036 - dis_loss: 12.9646 - joint_pred_loss: 16.0633 - accuracy: 0.0055 - class_accuracy: 0.0181 - domain_accuracy: 0.2404 - val_loss: 16.1181 - val_kl_loss: 0.0000e+00 - val_pred_loss: 16.1181 - val_dis_loss: 0.0000e+00 - val_joint_pred_loss: 16.1181 - val_accuracy: 0.0224 - val_class_accuracy: 0.0224 - val_domain_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    history = m.fit(\n",
    "        d.train_data(), \n",
    "        epochs=100,\n",
    "        callbacks=[early_stopping_monitor],\n",
    "        validation_data = d.valid_data()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: real_world_to_product_30/assets\n"
     ]
    }
   ],
   "source": [
    "m.save('real_world_to_product_30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "Encoder (Functional)         (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Identity (Tensor [(None, 2048)]            0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Identity_1 (Tens [(None, 2048)]            0         \n",
      "_________________________________________________________________\n",
      "Discriminator (Sequential)   (None, 2)                 4264962   \n",
      "=================================================================\n",
      "Total params: 27,852,674\n",
      "Trainable params: 27,799,554\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape=m.encoder.input_shape[1:])\n",
    "encoder = m.encoder(input)\n",
    "classifier = m.classifier(encoder)\n",
    "discriminator = grad_reverse(encoder)\n",
    "# discriminator = GradientReversal(1.0)(encoder)\n",
    "discriminator = m.discriminator(discriminator)\n",
    "\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate=1e-3,\n",
    "    decay_steps=100,\n",
    "    decay_rate=0.9)\n",
    "optimizer = SGD(learning_rate=lr_schedule, momentum=0.9)\n",
    "\n",
    "model = Model(inputs=[input], outputs=[discriminator])\n",
    "model.compile(\n",
    "    loss=[CategoricalCrossentropy()], \n",
    "    optimizer=optimizer, \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = d[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 224, 224, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 1.1227 - accuracy: 0.3281\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.3573 - accuracy: 0.8750\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.0466 - accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 9.1469e-04 - accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 3.7833e-04 - accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 8.8218e-05 - accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 9.4049e-05 - accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 7.0881e-05 - accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 2.2065e-05 - accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 2.2533e-05 - accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 2.7976e-05 - accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 1.9646e-05 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 5.1515e-05 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 2.3944e-05 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 1.1514e-05 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 5.7871e-06 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4.7089e-05 - accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-40d690255f1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1101\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \"\"\"\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    287\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    307\u001b[0m       \u001b[0mbatch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    340\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_supports_tf_logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1017\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    531\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \"\"\"\n\u001b[1;32m   1062\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1027\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(x, tf.reduce_sum(y, axis=2), epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.4076\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2974\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0756\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0253\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0091\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0027\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0017\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.1900e-04\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.9596e-04\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.2849e-04\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.4195e-04\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.8700e-04\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 9.3408e-05\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.9500e-05\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.2266e-05\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.7749e-04\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.4820e-05\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3433e-04\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 5.8295e-05\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.3132e-05\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 5.1926e-05\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.6782e-05\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 9.1006e-05\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.9571e-04\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 5.6156e-05\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 9.7555e-05\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3345e-04\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.7501e-05\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6671e-04\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 5.9056e-05\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.6020e-05\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 5.8065e-05\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.0358e-05\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.3095e-05\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.7655e-05\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.8857e-05\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0422e-05\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.0208e-05\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.2753e-05\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.9511e-05\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 9.4130e-05\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.8156e-05\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.5439e-05\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.3966e-05\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.3318e-05\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.9203e-05\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.5571e-05\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.5833e-05\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.0508e-05\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4144e-05\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6418e-05\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.0129e-04\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.0566e-05\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.0268e-05\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 5.2766e-05\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.3832e-05\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.0002e-04\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.9158e-05\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.2783e-05\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.8356e-05\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.7021e-05\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.0020e-04\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.6090e-05\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.0071e-04\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.3773e-05\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.7391e-05\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.1998e-05\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.0006e-05\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.3057e-05\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 5.1038e-05\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.1215e-05\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.2726e-05\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0057e-04\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 5.8580e-05\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.7296e-05\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.5009e-05\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 9.2306e-05\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9804e-05\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 5.9016e-05\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.6187e-05\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.9116e-05\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.1562e-05\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.1715e-05\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1052e-04\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 5.8024e-05\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.6238e-05\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 5.3100e-05\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.2513e-05\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.5004e-05\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2391e-04\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.2237e-05\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 9.1117e-05\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.1556e-05\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.5244e-05\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.0838e-05\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 9.2449e-05\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 7.2178e-05\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.7181e-05\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.1054e-05\n",
      "Epoch 100/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step - loss: 3.1593e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7faaae6e8580>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = m.discriminator\n",
    "model.compile(loss=CategoricalCrossentropy(), optimizer=optimizer)\n",
    "\n",
    "model.fit(m.encoder(x), tf.reduce_sum(y, axis=2), epochs=100)\n",
    "# model(m.encoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLLoss():\n",
    "    def __init__(self, classes):\n",
    "        self.classes = classes\n",
    "        self.__name__ = \"kl_loss\"\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        y_joint = tf.reshape(y_pred, (-1, 2*self.classes))\n",
    "\n",
    "        y_domain = tf.expand_dims(tf.reduce_sum(y_pred, axis=2), -1)\n",
    "        y_class = tf.expand_dims(tf.reduce_sum(y_pred, axis=1), 1)\n",
    "\n",
    "        y_ind_joint = tf.reshape((y_domain * y_class), (-1,2*self.classes))\n",
    "        return tf.keras.losses.KLDivergence()(\n",
    "                tf.transpose(y_joint), \n",
    "                tf.transpose(y_ind_joint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = d[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 2048]), (64, 2, 65))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.encoder(x).shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dom = tf.reduce_sum(y, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_loss = BinaryCrossentropy()(y_dom, m(x)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_loss = m.rep_loss(m(x)[0], m.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.60576>"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_loss + dis_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=0.06295085>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.5428091>)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_loss, dis_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(130,), dtype=int64, numpy=\n",
       "array([15, 23, 36, 28, 57, 16, 11, 14, 13, 35,  7, 57, 17, 54, 60, 58,  9,\n",
       "       16, 57, 54, 54, 61,  1, 14, 20, 40, 38, 22, 36, 44, 48, 28,  4, 37,\n",
       "       16, 32, 53,  1, 46,  4, 16, 31, 26, 45,  2, 32, 12, 61,  1,  4,  7,\n",
       "       18, 57, 15, 15,  0, 36, 36,  7,  6, 51,  9, 61, 12, 47, 14, 24, 18,\n",
       "       56,  2,  1, 28,  4, 38, 60, 14, 27, 51, 22, 57, 44, 31, 55, 22,  6,\n",
       "       46, 28,  6, 61, 13, 60, 45, 53, 28,  9, 13, 11, 40, 55,  2,  2, 22,\n",
       "       45, 31, 26,  4, 34, 54, 18, 51, 37, 27, 36, 23,  0, 51,  9, 61, 32,\n",
       "       62, 55, 49, 48, 18, 20, 31, 60, 24, 38, 10])>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.reshape(y, (-1, 2*m.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(130,), dtype=int64, numpy=\n",
       "array([26,  0,  4, 34, 22, 23,  0,  0,  0,  0,  0, 11, 30,  9,  0,  0,  0,\n",
       "        0,  0,  8,  0, 55,  7,  0,  0, 24,  2, 25,  0,  0, 45,  1,  6, 59,\n",
       "       29, 27,  0,  0, 13,  0, 35, 49,  0, 16, 42,  0, 50,  3, 60,  0,  0,\n",
       "       31, 17,  0, 57, 36, 28,  0, 14, 58,  0,  5,  0,  0, 20,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-257-ffd49b42124f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'summary'"
     ]
    }
   ],
   "source": [
    "m.discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=m.encoder.input_shape[1:])\n",
    "classifier_out = m.classifier(m.encoder(input_layer))\n",
    "discriminator_out = m.discriminator(m.encoder(input_layer))\n",
    "output_layer = [classifier_out, discriminator_out]\n",
    "model = Model(input_layer, output_layer)\n",
    "model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleDomainModel:\n",
    "    def __init__(self, \n",
    "                 classes=65):\n",
    "        self.input_shape = (224,224,3)\n",
    "        self.classes = classes\n",
    "        \n",
    "        self.model = self.build_model()\n",
    "        self.classifier = self.model.get_layer(\"Classifier\")\n",
    "        self.discriminator = self.model.get_layer(\"Discriminator\")\n",
    "        self.encoder = self.model.get_layer(\"Encoder\")\n",
    "    \n",
    "    def _build_encoder(self):\n",
    "        resnet = ResNet50(include_top=False, input_shape=self.input_shape)\n",
    "        pool = GlobalAveragePooling2D()(resnet.output)\n",
    "        return Model(inputs=resnet.input, outputs=pool, name=\"Encoder\")\n",
    "    \n",
    "    def _build_classifier(self, input_shape):\n",
    "        classifier = Sequential(name=\"Classifier\")\n",
    "        classifier.add(Dense(1280, \n",
    "                      input_shape=input_shape[1:],\n",
    "                      activation='relu'))\n",
    "        classifier.add(Dropout(0.5))\n",
    "        classifier.add(Dense(1280, activation='relu'))\n",
    "        classifier.add(Dropout(0.5))\n",
    "        classifier.add(Dense(\n",
    "            2 * self.classes, \n",
    "            activation='softmax'))\n",
    "        classifier.add(Reshape((2, self.classes)))\n",
    "        return classifier\n",
    "    \n",
    "    def _build_discriminator(self, input_shape):\n",
    "        discriminator = Sequential(name=\"Discriminator\")\n",
    "        discriminator.add(GradientReversal(1))\n",
    "        discriminator.add(Dense(1280, \n",
    "                      activation='relu'))\n",
    "        discriminator.add(Dropout(0.5))\n",
    "        discriminator.add(Dense(1280, activation='relu'))\n",
    "        discriminator.add(Dropout(0.5))\n",
    "        discriminator.add(Dense(\n",
    "            2, \n",
    "            activation='softmax'))\n",
    "        discriminator.build(input_shape)\n",
    "        return discriminator\n",
    "    \n",
    "    def build_model(self):\n",
    "        encoder = self._build_encoder()\n",
    "        classifier = self._build_classifier(encoder.output_shape)\n",
    "        discriminator = self._build_discriminator(encoder.output_shape)\n",
    "        \n",
    "        input_layer = Input(shape=encoder.input_shape[1:])\n",
    "        classifier_out = classifier(encoder(input_layer))\n",
    "        discriminator_out = discriminator(encoder(input_layer))\n",
    "        \n",
    "        output_layer = [classifier_out, discriminator_out]\n",
    "        return Model(input_layer, output_layer)\n",
    "        \n",
    "    def train(self, X, Ydomain, Yclass, epochs=100, batch_size=128):\n",
    "        for epoch in range(epochs):\n",
    "            pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_22\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Feature_Encoder (Functional)    (None, 2048)         23587712    input_19[0][0]                   \n",
      "                                                                 input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Joint_Classifier (Sequential)   (None, 4, 65)        4595460     Feature_Encoder[8][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Discriminator (Sequential)      (None, 4)            4267524     Feature_Encoder[9][0]            \n",
      "==================================================================================================\n",
      "Total params: 32,450,696\n",
      "Trainable params: 27,802,116\n",
      "Non-trainable params: 4,648,580\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330\n",
      "224\n",
      "212\n",
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len(model.weights))\n",
    "print(len(model.trainable_weights))\n",
    "print(len(model.get_layer(\"Feature_Encoder\").trainable_weights))\n",
    "print(len(model.get_layer(\"Joint_Classifier\").trainable_weights))\n",
    "print(len(model.get_layer(\"Discriminator\").trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = model.get_layer(\"Joint_Classifier\")\n",
    "d = model.get_layer(\"Discriminator\")\n",
    "f = model.get_layer(\"Feature_Encoder\")\n",
    "\n",
    "c.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_layer(\"Feature_Encoder\").trainable = True\n",
    "# model.get_layer(\"Feature_Encoder\").trainable = False\n",
    "# model.get_layer(\"Joint_Classifier\").trainable = True\n",
    "model.get_layer(\"Joint_Classifier\").trainable = False\n",
    "model.get_layer(\"Discriminator\").trainable = True\n",
    "# model.get_layer(\"Discriminator\").trainable = False\n",
    "# model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1280)              2622720   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1280)              1639680   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "Joint_Classifier (Dense)     (None, 260)               333060    \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4, 65)             0         \n",
      "=================================================================\n",
      "Total params: 4,595,460\n",
      "Trainable params: 4,595,460\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m.classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 1280)              2622720   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1280)              1639680   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "Domain_Discriminator (Dense) (None, 4)                 5124      \n",
      "=================================================================\n",
      "Total params: 4,267,524\n",
      "Trainable params: 4,267,524\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m.discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 23,534,592\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m.encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
